{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5441a8",
   "metadata": {},
   "source": [
    "# CLIP zero-shot visualization (Colab)\n",
    "\n",
    "This notebook runs the CLIP zero-shot example, displays the image with an overlaid confidence bar chart, and shows a 2D projection of the image and class embeddings. It also includes a small, visualization-only regularization that exaggerates close/far relationships in the latent space.\n",
    "\n",
    "Notes:\n",
    "- If you have a local `duck.jpg` file, upload it to the Colab runtime (click the Files tab -> Upload) or place it in the same directory.\n",
    "- The notebook will download a sample duck image automatically if no `duck.jpg` is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53a8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages. On Colab many of these are already available, but we include these commands to be explicit.\n",
    "# If you need GPU-enabled PyTorch, consider installing with the proper CUDA wheel for your runtime.\n",
    "!pip install -q transformers matplotlib pillow numpy\n",
    "# Optional: CPU-only PyTorch install (uncomment if needed)\n",
    "# !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d8805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and helpers\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"TypedStorage is deprecated\")\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('torch version:', torch.__version__, 'cuda available:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c4b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model + processor (this downloads weights on first run)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Using device:', device)\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32').to(device)\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "print('Model and processor loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e737808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path and fallback download if not present\n",
    "img_path = './duck.jpg'\n",
    "if not os.path.exists(img_path):\n",
    "    print('duck.jpg not found, downloading a sample image...')\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/4/49/Male_mallard_duck_head.jpg'\n",
    "    r = requests.get(url)\n",
    "    open(img_path, 'wb').write(r.content)\n",
    "    print('Downloaded sample image to', img_path)\n",
    "\n",
    "# Candidate labels (edit as desired)\n",
    "labels = [\n",
    "    'a picture of a bull','a picture of a racecar',\n",
    "    'a picture of a woman', 'a picture of a man',\n",
    "    'a picture of a queen', 'a picture of a king', 'a photo of donald duck', \n",
    "    'a photo of a duck', 'a photo of a bird'\n",
    "]\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6950bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs and run CLIP\n",
    "image = Image.open(img_path)\n",
    "inputs = processor(text=labels, images=image, return_tensors='pt', padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "best_idx = probs[0].argmax().item()\n",
    "print(f'Predicted label: {labels[best_idx]} (confidence {probs[0][best_idx]:.2f})')\n",
    "for i, label in enumerate(labels):\n",
    "    if i != best_idx:\n",
    "        print(f'Other label: {label} (confidence {probs[0][i]:.2f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08dd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: show image and overlay confidence bar chart\n",
    "# Ensure image is RGB and convert for plotting\n",
    "image_rgb = Image.open(img_path).convert('RGB')\n",
    "image_np = np.asarray(image_rgb)\n",
    "confidences = probs[0].cpu().numpy()\n",
    "order = np.argsort(confidences)[::-1]\n",
    "ordered_conf = confidences[order]\n",
    "ordered_labels = [labels[i] for i in order]\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "fig.subplots_adjust(left=0.18, right=0.98, top=0.95, bottom=0.04)\n",
    "ax_img = fig.add_axes([0.05, 0.28, 0.9, 0.67])\n",
    "ax_img.imshow(image_np)\n",
    "ax_img.axis('off')\n",
    "ax_bar = fig.add_axes([0.18, 0.03, 0.78, 0.17])\n",
    "ax_bar.patch.set_alpha(0.7)\n",
    "y_pos = np.arange(len(ordered_labels))\n",
    "ax_bar.barh(y_pos, ordered_conf, color='C0', height=0.6)\n",
    "ax_bar.set_yticks(y_pos)\n",
    "ax_bar.set_yticklabels(ordered_labels, fontsize=10)\n",
    "ax_bar.invert_yaxis()\n",
    "ax_bar.set_xlim(0,1)\n",
    "ax_bar.set_xlabel('Confidence')\n",
    "for i, v in enumerate(ordered_conf):\n",
    "    x_pos = min(v + 0.02, 0.99)\n",
    "    ax_bar.text(x_pos, i, f\"{v:.2f}\", va='center', fontsize=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f3ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding visualization with optional regularization\n",
    "# Extract embeddings (prefer outputs if provided)\n",
    "try:\n",
    "    image_embed = outputs.image_embeds[0]\n",
    "    text_embeds = outputs.text_embeds\n",
    "except Exception:\n",
    "    with torch.no_grad():\n",
    "        if 'pixel_values' in inputs:\n",
    "            image_embed = model.get_image_features(pixel_values=inputs['pixel_values'].to(device))[0]\n",
    "        else:\n",
    "            raise RuntimeError('No image tensor found for embedding computation')\n",
    "        if 'input_ids' in inputs:\n",
    "            text_embeds = model.get_text_features(input_ids=inputs['input_ids'].to(device), attention_mask=inputs.get('attention_mask', None).to(device))\n",
    "        else:\n",
    "            raise RuntimeError('No text tensor found for embedding computation')\n",
    "\n",
    "img_e = image_embed.detach().cpu().numpy()\n",
    "txt_e = text_embeds.detach().cpu().numpy()\n",
    "\n",
    "from numpy.linalg import norm\n",
    "distances = np.array([norm(img_e - t) for t in txt_e])\n",
    "\n",
    "# Visualization-only regularization function\n",
    "def regularize_text_embeddings(txt_embeddings, img_embedding, power=1.8, eps=1e-8):\n",
    "    dists = np.linalg.norm(txt_embeddings - img_embedding, axis=1)\n",
    "    mean_dist = dists.mean() + eps\n",
    "    ratios = (dists / mean_dist)\n",
    "    multipliers = (ratios ** power)\n",
    "    multipliers = np.nan_to_num(multipliers, nan=1.0, posinf=ratios.max(), neginf=1.0)\n",
    "    dirs = txt_embeddings - img_embedding[np.newaxis, :]\n",
    "    transformed = img_embedding[np.newaxis, :] + dirs * multipliers[:, np.newaxis]\n",
    "    return transformed\n",
    "\n",
    "power = 1.8  # change this to exaggerate or reduce separation\n",
    "txt_e_reg = regularize_text_embeddings(txt_e, img_e, power=power)\n",
    "reg_distances = np.array([norm(img_e - t) for t in txt_e_reg])\n",
    "\n",
    "# PCA via SVD to 2D for plotting\n",
    "all_emb = np.vstack([img_e[np.newaxis, :], txt_e_reg])\n",
    "all_emb_mean = all_emb.mean(axis=0)\n",
    "X = all_emb - all_emb_mean\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "coords = X.dot(Vt.T[:, :2])\n",
    "img_xy = coords[0]\n",
    "txt_xy = coords[1:]\n",
    "\n",
    "fig2 = plt.figure(figsize=(10,5))\n",
    "ax_emb = fig2.add_axes([0.06, 0.10, 0.88, 0.85])\n",
    "normed = (reg_distances - reg_distances.min()) / (np.ptp(reg_distances) + 1e-8)\n",
    "colors = plt.cm.viridis(1 - normed)\n",
    "ax_emb.scatter(txt_xy[:, 0], txt_xy[:, 1], c=colors, s=90, edgecolor='k')\n",
    "ax_emb.scatter(img_xy[0], img_xy[1], marker='*', s=220, c='red', edgecolor='k', label='image')\n",
    "for i, label in enumerate(labels):\n",
    "    x1, y1 = img_xy\n",
    "    x2, y2 = txt_xy[i]\n",
    "    ax_emb.plot([x1, x2], [y1, y2], color='gray', linewidth=0.8, linestyle='--')\n",
    "    mx, my = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    ax_emb.text(mx, my, f\"{reg_distances[i]:.2f}\", fontsize=9, color='black', bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))\n",
    "    ax_emb.text(txt_xy[i, 0] + 0.01, txt_xy[i, 1] + 0.01, label, fontsize=9)\n",
    "\n",
    "ax_emb.set_title(f'2D projection of CLIP embeddings (power={power:.2f} regularization)')\n",
    "ax_emb.axis('equal')\n",
    "ax_emb.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e91c6d",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "- Adjust `power` in the embedding cell to control how much nearby embeddings are pulled closer and far ones pushed farther for visualization.\n",
    "- If you want to compare original vs regularized spaces, I can add a side-by-side plot.\n",
    "- To save figures automatically, you can add `fig.savefig('foo.png', dpi=200)` after the plotting cells."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
